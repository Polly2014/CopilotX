# üöÄ CopilotX

Local & Remote GitHub Copilot API proxy ‚Äî use GPT-4o, Claude, Gemini and more via OpenAI/Anthropic compatible APIs.

Turn your GitHub Copilot subscription into an AI API server. Use **any model** available through Copilot with **any tool** that supports OpenAI or Anthropic SDKs ‚Äî locally or on a remote VM.

## ‚ú® Features

- üîê **GitHub OAuth** ‚Äî One-command login via Device Flow, or use existing token
- üîÑ **Auto Token Refresh** ‚Äî Copilot JWT refreshed transparently before expiry
- üîå **Dual API Format** ‚Äî OpenAI `/v1/chat/completions` + Anthropic `/v1/messages`
- üåä **SSE Streaming** ‚Äî Real-time streaming responses for both formats
- üìã **Model Discovery** ‚Äî Auto-fetch available models from Copilot
- ‚ö° **Zero Config** ‚Äî `pip install` ‚Üí `auth login` ‚Üí `serve` ‚Üí done
- üåê **Remote Deploy** ‚Äî Serve on `0.0.0.0` with API key protection, deploy behind Caddy for auto-HTTPS

## üöÄ Quick Start

### 1. Install

```bash
pip install copilotx
# or
uv pip install copilotx
```

### 2. Authenticate

```bash
# Option A: OAuth Device Flow (recommended)
copilotx auth login
# ‚Üí Opens browser for GitHub authorization

# Option B: Use existing GitHub token
copilotx auth login --token ghp_xxxxx
# or
export GITHUB_TOKEN=ghp_xxxxx && copilotx auth login
```

### 3. Start Server

```bash
copilotx serve
```

Output:
```
üöÄ CopilotX v2.0.0
‚úÖ Copilot Token valid (28m remaining, auto-refresh)
üè† Local mode (localhost only)
üìã Models: gpt-4o, gpt-4o-mini, o3-mini, claude-sonnet-4, gemini-2.0-flash

üîó OpenAI API:    http://127.0.0.1:24680/v1/chat/completions
üîó Anthropic API: http://127.0.0.1:24680/v1/messages
üîó Models:        http://127.0.0.1:24680/v1/models

Press Ctrl+C to stop
```

### 4. Use It

**Python (OpenAI SDK):**

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:24680/v1", api_key="copilotx")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True,
)

for chunk in response:
    print(chunk.choices[0].delta.content or "", end="")
```

**Python (Anthropic SDK):**

```python
from anthropic import Anthropic

client = Anthropic(base_url="http://localhost:24680", api_key="copilotx")

message = client.messages.create(
    model="claude-sonnet-4",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}],
)
print(message.content[0].text)
```

**Claude Code:**

```bash
# Set environment variables
export ANTHROPIC_BASE_URL=http://localhost:24680
export ANTHROPIC_API_KEY=copilotx
claude
```

**Codex:**

```bash
export OPENAI_BASE_URL=http://localhost:24680/v1
export OPENAI_API_KEY=copilotx
codex
```

**cURL:**

```bash
curl http://localhost:24680/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## üì° API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/chat/completions` | POST | OpenAI-compatible chat completions |
| `/v1/messages` | POST | Anthropic-compatible messages |
| `/v1/models` | GET | List available models |
| `/health` | GET | Server health + token status |

## üîß CLI Commands

```bash
copilotx auth login              # OAuth Device Flow login
copilotx auth login --token XXX  # Quick login with existing token
copilotx auth status             # Show auth status
copilotx auth logout             # Clear credentials

copilotx models                  # List available models
copilotx serve                   # Start server (default: 127.0.0.1:24680)
copilotx serve --host 0.0.0.0   # Remote mode (bind all interfaces)
copilotx serve --port 9090       # Custom port (strict ‚Äî fails if in use)
copilotx --version               # Show version
```

## üèóÔ∏è How It Works

```
Your Tool (Claude Code / Codex / Python script)
    ‚îÇ
    ‚îÇ  OpenAI or Anthropic format
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CopilotX (localhost:24680)  ‚îÇ
‚îÇ                              ‚îÇ
‚îÇ  ‚Ä¢ Anthropic ‚Üí OpenAI        ‚îÇ
‚îÇ    format translation        ‚îÇ
‚îÇ  ‚Ä¢ Token auto-refresh        ‚îÇ
‚îÇ  ‚Ä¢ SSE stream forwarding     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ  OpenAI format
               ‚ñº
  api.githubcopilot.com/chat/completions
  (GPT-4o, Claude, Gemini, o3-mini, ...)
```

CopilotX uses your GitHub Copilot subscription to access models. The Copilot backend
natively speaks OpenAI format, so OpenAI requests are **direct passthrough**. Anthropic
requests are translated on-the-fly.

## üîç Port Discovery

When CopilotX starts, it writes `~/.copilotx/server.json`:

```json
{
  "host": "127.0.0.1",
  "port": 24680,
  "pid": 12345,
  "started_at": "2026-02-09T12:00:00+00:00",
  "base_url": "http://127.0.0.1:24680"
}
```

Other scripts can read this to discover the actual port:

```bash
# Bash/Zsh
PORT=$(python -c "import json; print(json.load(open('$HOME/.copilotx/server.json'))['port'])")
curl http://localhost:$PORT/health

# PowerShell
$info = Get-Content "$HOME\.copilotx\server.json" | ConvertFrom-Json
curl http://localhost:$($info.port)/health
```

The file is automatically cleaned up when the server stops.

## üåê Remote Deployment

Deploy CopilotX on a cloud VM to access your Copilot models from anywhere.

### Quick Setup (Azure VM / any Linux server)

```bash
# 1. Install
pip install copilotx

# 2. Authenticate
copilotx auth login

# 3. Set API key for remote protection
export COPILOTX_API_KEY=$(openssl rand -hex 32)
echo "Save this key: $COPILOTX_API_KEY"

# 4. Start in remote mode
copilotx serve --host 0.0.0.0
```

### Production Setup with Nginx + systemd

For production deployments with HTTPS, we recommend using Nginx as the reverse proxy.

**1. Install and configure systemd service:**

```bash
# Copy and customize the systemd service template
sudo cp deploy/copilotx.service /etc/systemd/system/

# Create environment file with your API key
mkdir -p ~/.copilotx
echo "COPILOTX_API_KEY=$(openssl rand -hex 32)" > ~/.copilotx/.env

# Enable and start service
sudo systemctl daemon-reload
sudo systemctl enable --now copilotx
```

**2. Configure Nginx reverse proxy:**

```bash
# Copy the Nginx config template
sudo cp deploy/nginx-copilotx.conf /etc/nginx/sites-available/copilotx
sudo ln -s /etc/nginx/sites-available/copilotx /etc/nginx/sites-enabled/

# Get SSL certificate with Let's Encrypt
sudo certbot --nginx -d your-domain.com

# Reload Nginx
sudo nginx -t && sudo systemctl reload nginx
```

The `deploy/` directory includes ready-to-use templates:
- `copilotx.service` ‚Äî systemd service unit
- `nginx-copilotx.conf` ‚Äî Nginx reverse proxy with SSL, rate limiting, and SSE support
- `Caddyfile` ‚Äî Alternative Caddy config (simpler setup with auto-HTTPS)

### Security Model

| Mode | Host | API Key | Behavior |
|------|------|---------|----------|
| **Local** | `127.0.0.1` (default) | Not needed | Fully open, localhost only |
| **Remote (protected)** | `0.0.0.0` | `COPILOTX_API_KEY` set | Localhost exempt, remote needs Bearer token |
| **Remote (open)** | `0.0.0.0` | Not set | ‚ö†Ô∏è Warning shown, fully open |

**Accessing from remote:**

```bash
# Use Bearer token
curl https://your-domain.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"

# Or x-api-key header
curl https://your-domain.com/v1/models \
  -H "x-api-key: YOUR_API_KEY"
```

**With OpenAI SDK:**

```python
client = OpenAI(
    base_url="https://your-domain.com/v1",
    api_key="YOUR_COPILOTX_API_KEY",
)
```

## üìã Version Roadmap

| Version | Codename | Features |
|---------|----------|----------|
| v1.0.0 | Local | OAuth, dual API, streaming, model discovery |
| **v2.0.0** | **Remote** | **API key auth, remote deploy, Nginx/Caddy + systemd templates** |
| v3.0.0 | Multi-User | Token pool, user database, OpenRouter mode |

## ‚ö†Ô∏è Disclaimer

This tool is for **personal local use only**. Please comply with
[GitHub Copilot Terms of Service](https://docs.github.com/en/copilot/overview-of-github-copilot/about-github-copilot-individual).
The author is not responsible for any account restrictions resulting from misuse.

## üìÑ License

MIT
